version: '3.9'

services:
  # chromadb:
  #   image: chromadb/chroma:latest
  #   volumes:
  #     - ./chromadb:/chroma/chroma
  #   command: "--workers 1 --host 0.0.0.0 --port 8000 --proxy-headers --log-config chromadb/log_config.yml --timeout-keep-alive 30"
  #   environment:
  #     - IS_PERSISTENT=TRUE
  #   restart: unless-stopped
  #   ports:
  #     - "4810:8000"
  #   healthcheck:
  #     test: [ "CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat" ]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  llamacpp-server:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    ports:
      - 4820:4820
    volumes:
      - ./models:/models
    environment:
      LD_LIBRARY_PATH: /usr/local/cuda/compat/ # https://github.com/ggerganov/llama.cpp/issues/7822
      LLAMA_ARG_MODEL: /models/LaBSE-q2_K.gguf
      LLAMA_ARG_N_PARALLEL: 3
      LLAMA_ARG_PORT: 4820
      LLAMA_ARG_EMBEDDINGS: TRUE
      LLAMA_ARG_N_GPU_LAYERS: -1

# https://github.com/ggerganov/llama.cpp/discussions/8430